{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9998410,"sourceType":"datasetVersion","datasetId":6153996},{"sourceId":9998550,"sourceType":"datasetVersion","datasetId":6154099}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom transformers import RobertaTokenizer, RobertaModel\nimport pandas as pd\nfrom PIL import Image\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:10:34.876371Z","iopub.execute_input":"2024-11-24T10:10:34.876730Z","iopub.status.idle":"2024-11-24T10:10:34.881346Z","shell.execute_reply.started":"2024-11-24T10:10:34.876697Z","shell.execute_reply":"2024-11-24T10:10:34.880445Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"! pip install -q gdown\n! pip install -q torchmetrics\n\n!wget --no-check-certificate -O bully_data.zip \"https://iitgoffice-my.sharepoint.com/:u:/g/personal/sjana_iitg_ac_in/EWdMrS9zHgBHnz4TTckw14kB14O8j0IbR_D-fBowyw7T7A?e=8H4Wpw&download=1\"\n\n!gdown \"https://drive.google.com/uc?id=1RpvrH0OJXBnJDzAFerqBb7v1ZKLbxTG8\"\n!gdown \"https://drive.google.com/uc?id=19oLWMCuVHye63YvKBrG7-EbW4OU_VQOQ\"\n\n!rm -rf bully_data/\n!mkdir bully_data\n!mkdir bully_data/data\n#!mkdir bully_data/test\n!unzip bully_data.zip -d bully_data/data/ | tqdm >/dev/null\n!unzip Cyberbully_corrected_emotion_sentiment_v2.zip -d bully_data/ | tqdm >/dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:17:44.536435Z","iopub.execute_input":"2024-11-24T10:17:44.536771Z","iopub.status.idle":"2024-11-24T10:18:25.405719Z","shell.execute_reply.started":"2024-11-24T10:17:44.536742Z","shell.execute_reply":"2024-11-24T10:18:25.404841Z"}},"outputs":[{"name":"stdout","text":"--2024-11-24 10:18:02--  https://iitgoffice-my.sharepoint.com/:u:/g/personal/sjana_iitg_ac_in/EWdMrS9zHgBHnz4TTckw14kB14O8j0IbR_D-fBowyw7T7A?e=8H4Wpw&download=1\nResolving iitgoffice-my.sharepoint.com (iitgoffice-my.sharepoint.com)... 13.107.138.10, 13.107.136.10, 2620:1ec:8fa::10, ...\nConnecting to iitgoffice-my.sharepoint.com (iitgoffice-my.sharepoint.com)|13.107.138.10|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /personal/sjana_iitg_ac_in/Documents/bully_data-20240404T103912Z-001.zip?ga=1 [following]\n--2024-11-24 10:18:03--  https://iitgoffice-my.sharepoint.com/personal/sjana_iitg_ac_in/Documents/bully_data-20240404T103912Z-001.zip?ga=1\nReusing existing connection to iitgoffice-my.sharepoint.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 124138703 (118M) [application/x-zip-compressed]\nSaving to: 'bully_data.zip'\n\nbully_data.zip      100%[===================>] 118.39M  28.7MB/s    in 4.3s    \n\n2024-11-24 10:18:08 (27.5 MB/s) - 'bully_data.zip' saved [124138703/124138703]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1RpvrH0OJXBnJDzAFerqBb7v1ZKLbxTG8\nTo: /kaggle/working/Cyberbully_corrected_emotion_sentiment.zip\n100%|█████████████████████████████████████████| 411k/411k [00:00<00:00, 107MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=19oLWMCuVHye63YvKBrG7-EbW4OU_VQOQ\nTo: /kaggle/working/Cyberbully_corrected_emotion_sentiment_v2.zip\n100%|█████████████████████████████████████████| 422k/422k [00:00<00:00, 111MB/s]\n6008it [00:01, 5357.70it/s]\n2it [00:00, 52428.80it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\nimport pickle\n\nexcel_path = \"/kaggle/working/bully_data/data/bully_data/Copy of Cyberbully_corrected_emotion_sentiment.xlsx\"\nimage_path = \"/kaggle/working/bully_data/data/bully_data/\"\n\n# Load the Excel file\ndf = pd.read_excel(excel_path)\ndf = df.drop(columns=['Unnamed: 10', 'Unnamed: 11'])\n# Display the first few rows to ensure it is loaded correctly\n# print(df)\n\ndf_cleaned = df.dropna()\ndf=df_cleaned\n# print(df)\nimport os\nimport pandas as pd\n\n# Assuming your DataFrame is called df and contains the 'img_id' column\n# Assuming image paths are in a directory (img_dir) and filenames correspond to 'img_id'\n\nimg_dir = image_path\n# Define a function to check if the image size is zero\ndef is_zero_size(img_id, img_dir):\n    img_path = os.path.join(img_dir, img_id)\n    return os.path.exists(img_path) and os.path.getsize(img_path) == 0\n\n# Filter out rows with zero-size images\ndf['is_zero_size'] = df['Img_Name'].apply(lambda img_id: is_zero_size(img_id, img_dir))\ndf_filtered = df[df['is_zero_size'] == False].drop(columns='is_zero_size')\n\n# Now, df_filtered contains only rows with non-zero-size images\n# print(df_filtered)\ndf=df_filtered\ndf_cleaned = df[df['Img_Name'] != '2644.jpg']\n# df_cleaned = df[df['Img_Name'] != '3805.jpg']\ndf=df_cleaned\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:14.074294Z","iopub.execute_input":"2024-11-24T10:20:14.074621Z","iopub.status.idle":"2024-11-24T10:20:15.298305Z","shell.execute_reply.started":"2024-11-24T10:20:14.074591Z","shell.execute_reply":"2024-11-24T10:20:15.297347Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:18.249649Z","iopub.execute_input":"2024-11-24T10:20:18.250004Z","iopub.status.idle":"2024-11-24T10:20:18.255241Z","shell.execute_reply.started":"2024-11-24T10:20:18.249973Z","shell.execute_reply":"2024-11-24T10:20:18.254445Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(3078, 10)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"emotion_mapping = {\n    \"Disgust\": \"Negative\",\n    \"Ridicule\": \"Negative\",\n    \"Sadness\": \"Negative\",\n    \"Surprise\": \"Neutral\",\n    \"Anticipation\": \"Positive\",\n    \"Angry\": \"Negative\",\n    \"Happiness\": \"Positive\",\n    \"Other\": \"Neutral\",\n    \"Trust\": \"Positive\",\n    \"Fear\": \"Negative\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:24.673720Z","iopub.execute_input":"2024-11-24T10:20:24.674662Z","iopub.status.idle":"2024-11-24T10:20:24.678786Z","shell.execute_reply.started":"2024-11-24T10:20:24.674607Z","shell.execute_reply":"2024-11-24T10:20:24.677818Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df['Emotion'] = df['Emotion'].map(emotion_mapping)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:26.882025Z","iopub.execute_input":"2024-11-24T10:20:26.882798Z","iopub.status.idle":"2024-11-24T10:20:26.888489Z","shell.execute_reply.started":"2024-11-24T10:20:26.882766Z","shell.execute_reply":"2024-11-24T10:20:26.887587Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/4169271968.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Emotion'] = df['Emotion'].map(emotion_mapping)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:32.363065Z","iopub.execute_input":"2024-11-24T10:20:32.363832Z","iopub.status.idle":"2024-11-24T10:20:32.368134Z","shell.execute_reply.started":"2024-11-24T10:20:32.363799Z","shell.execute_reply":"2024-11-24T10:20:32.367320Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:36.026516Z","iopub.execute_input":"2024-11-24T10:20:36.027211Z","iopub.status.idle":"2024-11-24T10:20:36.032509Z","shell.execute_reply.started":"2024-11-24T10:20:36.027177Z","shell.execute_reply":"2024-11-24T10:20:36.031601Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(3078, 10)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:20:38.684111Z","iopub.execute_input":"2024-11-24T10:20:38.684601Z","iopub.status.idle":"2024-11-24T10:20:38.696080Z","shell.execute_reply.started":"2024-11-24T10:20:38.684573Z","shell.execute_reply":"2024-11-24T10:20:38.695250Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"  Img_Name                                           Img_Text Img_Text_Label  \\\n0    0.jpg  Shivam @shivamishraa Girls be named naina and ...          Bully   \n2    2.jpg     For Boyfriend For Bestfriend DESI ADUKT TROLLS          Bully   \n4    4.jpg  not_shubham14 @mentally_dank Kids at Marine Dr...          Bully   \n5    5.jpg                  what if we use 100% of our brain?          Bully   \n6    6.jpg  If the opposite of Con is Pro Is Congress the ...          Bully   \n\n  Img_Label Text_Label Sentiment   Emotion Sarcasm      Harmful_Score  \\\n0  Nonbully      Bully  Negative  Negative     Yes  Partially-Harmful   \n2     Bully   Nonbully  Negative  Negative      No  Partially-Harmful   \n4  Nonbully   Nonbully  Negative  Negative      No  Partially-Harmful   \n5     Bully   Nonbully  Negative   Neutral      No  Partially-Harmful   \n6  Nonbully      Bully  Negative  Negative      No  Partially-Harmful   \n\n         Target  \n0    Individual  \n2       Society  \n4    Individual  \n5    Individual  \n6  Organization  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Img_Name</th>\n      <th>Img_Text</th>\n      <th>Img_Text_Label</th>\n      <th>Img_Label</th>\n      <th>Text_Label</th>\n      <th>Sentiment</th>\n      <th>Emotion</th>\n      <th>Sarcasm</th>\n      <th>Harmful_Score</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>Shivam @shivamishraa Girls be named naina and ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Negative</td>\n      <td>Yes</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>For Boyfriend For Bestfriend DESI ADUKT TROLLS</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Negative</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Society</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.jpg</td>\n      <td>not_shubham14 @mentally_dank Kids at Marine Dr...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Negative</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.jpg</td>\n      <td>what if we use 100% of our brain?</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Neutral</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.jpg</td>\n      <td>If the opposite of Con is Pro Is Congress the ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Negative</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Organization</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"class MemeDataset(Dataset):\n    def __init__(self, dataframe, transform):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        self.img_folder = image_path\n        # Define label mappings\n        self.text_label_mapping = {\n            \"Bully\": 1,\n            \"Nonbully\": 0\n        }\n        \n        self.sentiment_mapping = {\n            \"Positive\":1,\n            \"Neutral\": 0,\n            \"Negative\": 2\n        }\n        \n        # self.emotion_mapping = {\n        #     \"Disgust\": 0,\n        #     \"Ridicule\": 1,\n        #     \"Sadness\": 2,\n        #     \"Surprise\": 3,\n        #     \"Anticipation\": 4,\n        #     \"Angry\": 5,\n        #     \"Happiness\": 6,\n        #     \"Other\": 7,\n        #     \"Trust\": 8,\n        #     \"Fear\": 9\n        # }\n        \n        self.emotion_mapping = {\n            \"Positive\":1,\n            \"Neutral\": 0,\n            \"Negative\": 2\n        }\n        self.sarcasm_mapping = {\n            \"Yes\": 1,\n            \"No\": 0\n        }\n        \n        self.harmful_score_mapping = {\n            \"Harmless\": 0,\n            \"Partially-Harmful\": 1,\n            \"Very-Harmful\": 2\n        }\n        \n        self.target_mapping = {\n            \"Individual\": 0,\n            \"Society\": 1,\n            \"Organization\": 2,\n            \"Community\": 3\n        }\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_name = self.dataframe.iloc[idx]['Img_Name']\n        img_path = os.path.join(self.img_folder, img_name)\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        # Load and tokenize text\n        text = self.dataframe.iloc[idx]['Img_Text']\n        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n        \n        # Get labels and apply mappings\n        sentiment_label = torch.tensor(self.sentiment_mapping[self.dataframe.iloc[idx]['Sentiment']], dtype=torch.long)\n        emotion_label = torch.tensor(self.emotion_mapping[self.dataframe.iloc[idx]['Emotion']], dtype=torch.long)\n        sarcasm_label = torch.tensor(self.sarcasm_mapping[self.dataframe.iloc[idx]['Sarcasm']], dtype=torch.float)  # Binary sarcasm\n        bully_label = torch.tensor(self.text_label_mapping[self.dataframe.iloc[idx]['Img_Label']], dtype=torch.long)  # Bully detection\n        harmful_score_label = torch.tensor(self.harmful_score_mapping[self.dataframe.iloc[idx]['Harmful_Score']], dtype=torch.long)\n        target_label = torch.tensor(self.target_mapping[self.dataframe.iloc[idx]['Target']], dtype=torch.long)\n        \n        return image, inputs['input_ids'].squeeze(), inputs['attention_mask'].squeeze(), sentiment_label, emotion_label, sarcasm_label, bully_label, harmful_score_label, target_label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:25:13.228137Z","iopub.execute_input":"2024-11-24T10:25:13.228445Z","iopub.status.idle":"2024-11-24T10:25:13.239586Z","shell.execute_reply.started":"2024-11-24T10:25:13.228419Z","shell.execute_reply":"2024-11-24T10:25:13.238738Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class Image_text(nn.Module):\n    def __init__(self):\n        super(Image_text, self).__init__()\n        \n        # Visual branch (CNN)\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n        \n        # Textual branch (RoBERTa)\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        \n        # Shared fully connected layers\n        self.fc_shared = nn.Sequential(\n            nn.Linear(2048 + 768, 1024),  # Concatenation of visual and textual features\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Task-specific heads\n        self.sentiment_head = nn.Linear(512, 3)  # Sentiment: 3 classes\n        self.emotion_head = nn.Linear(512, 10)    # Emotion: 6 classes\n        self.sarcasm_head = nn.Linear(512, 1)    # Sarcasm: binary classification\n        self.bully_head = nn.Linear(512, 2)      # Cyberbullying: 2 classes\n        self.harmful_head = nn.Linear(512, 3)\n        self.target_head = nn.Linear(512, 4)\n    def forward(self, image, text_input_ids, text_attention_mask):\n        # Visual features\n        img_features = self.resnet(image)\n        \n        # Textual features\n        text_outputs = self.roberta(input_ids=text_input_ids, attention_mask=text_attention_mask)\n        text_features = text_outputs.pooler_output\n        \n        # Concatenate the visual and textual features\n        combined_features = torch.cat((img_features, text_features), dim=1)\n        \n        # Shared layers\n        shared_out = self.fc_shared(combined_features)\n        \n        # Task-specific outputs\n        sentiment_out = self.sentiment_head(shared_out)\n        emotion_out = self.emotion_head(shared_out)\n        sarcasm_out = torch.sigmoid(self.sarcasm_head(shared_out))\n        bully_out = self.bully_head(shared_out)\n        harmful_out = self.harmful_head(shared_out)\n        target_out = self.target_head(shared_out)\n        \n        return sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_out, target_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:21:11.025390Z","iopub.execute_input":"2024-11-24T10:21:11.025665Z","iopub.status.idle":"2024-11-24T10:21:11.033942Z","shell.execute_reply.started":"2024-11-24T10:21:11.025629Z","shell.execute_reply":"2024-11-24T10:21:11.033018Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class Image_text_emotion(nn.Module):\n    def __init__(self):\n        super(Image_text_emotion, self).__init__()\n        \n        # Visual branch (CNN)\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n        \n        # Textual branch (RoBERTa)\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        \n        # Shared fully connected layers\n        self.fc_shared = nn.Sequential(\n            nn.Linear(2048 + 768, 1024),  # Concatenation of visual and textual features\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Task-specific heads\n        self.sentiment_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # Sentiment: 3 classes\n        )\n        \n        self.emotion_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # Emotion: 3 classes\n        )\n        \n        self.sarcasm_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),  # Sarcasm: binary classification\n            nn.Sigmoid()  # Ensures output is in (0, 1)\n        )\n        # self.bully_head = nn.Linear(512, 2)  # Bully: binary classification\n        self.bully_fc = nn.Sequential(\n            nn.Linear(3 + 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2)  # 2 classes for bully\n        )\n        # self.harmful_head = nn.Linear(512, 3)  # Harmful score: 3 classes\n        self.harmful_fc = nn.Sequential(\n            nn.Linear(3 + 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # 3 classes for harmful\n        )\n        # Final target head\n        self.target_fc = nn.Sequential(\n            nn.Linear(3 + 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 4)  # 4 classes for Target\n        )\n        \n    def forward(self, image, text_input_ids, text_attention_mask):\n        # Visual features\n        img_features = self.resnet(image)\n        \n        # Textual features\n        text_outputs = self.roberta(input_ids=text_input_ids, attention_mask=text_attention_mask)\n        text_features = text_outputs.pooler_output\n        \n        # Concatenate the visual and textual features\n        combined_features = torch.cat((img_features, text_features), dim=1)\n        \n        # Shared features\n        shared_out = self.fc_shared(combined_features)\n        \n        # Task-specific predictions\n        sentiment_out = self.sentiment_head(shared_out)\n        emotion_out = self.emotion_head(shared_out)\n        sarcasm_out = torch.sigmoid(self.sarcasm_head(shared_out))  # Binary\n        # bully_out = self.bully_head(shared_out)\n        # harmful_out = self.harmful_head(shared_out)\n        \n        # Concatenate all task outputs with shared features for target prediction\n        aux_features = torch.cat((\n            emotion_out,\n            shared_out  # Shared features\n        ), dim=1)\n        \n        # Final target prediction\n        bully_out = self.bully_fc(aux_features)\n        harmful_out = self.harmful_fc(aux_features)\n        target_out = self.target_fc(aux_features)\n        \n        return sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_out, target_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:56:26.081521Z","iopub.execute_input":"2024-11-24T11:56:26.082447Z","iopub.status.idle":"2024-11-24T11:56:26.093507Z","shell.execute_reply.started":"2024-11-24T11:56:26.082411Z","shell.execute_reply":"2024-11-24T11:56:26.092794Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"class Image_text_sentiment(nn.Module):\n    def __init__(self):\n        super(Image_text_sentiment, self).__init__()\n        \n        # Visual branch (CNN)\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n        \n        # Textual branch (RoBERTa)\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        \n        # Shared fully connected layers\n        self.fc_shared = nn.Sequential(\n            nn.Linear(2048 + 768, 1024),  # Concatenation of visual and textual features\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Task-specific heads\n        self.sentiment_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # Sentiment: 3 classes\n        )\n        \n        self.emotion_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # Emotion: 3 classes\n        )\n        \n        self.sarcasm_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),  # Sarcasm: binary classification\n            nn.Sigmoid()  # Ensures output is in (0, 1)\n        )\n        \n        self.bully_fc = nn.Sequential(\n            nn.Linear(3 + 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 2)  # 2 classes for bully\n        )\n        \n        self.harmful_fc = nn.Sequential(\n            nn.Linear(3+ 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 3)  # 3 classes for harmful\n        )\n        \n        self.target_fc = nn.Sequential(\n            nn.Linear(3 + 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 4)  # 4 classes for Target\n        )\n        \n    def forward(self, image, text_input_ids, text_attention_mask):\n        # Visual features\n        img_features = self.resnet(image)\n        \n        # Textual features\n        text_outputs = self.roberta(input_ids=text_input_ids, attention_mask=text_attention_mask)\n        text_features = text_outputs.pooler_output\n        \n        # Concatenate the visual and textual features\n        combined_features = torch.cat((img_features, text_features), dim=1)\n        \n        # Shared features\n        shared_out = self.fc_shared(combined_features)\n        \n        # Task-specific predictions\n        sentiment_out = self.sentiment_head(shared_out)\n        emotion_out = self.emotion_head(shared_out)\n        sarcasm_out = self.sarcasm_head(shared_out)\n        \n        # Concatenate all task outputs with shared features for target prediction\n        aux_features = torch.cat((\n            sentiment_out,\n            shared_out  # Shared features\n        ), dim=1)\n        \n        # Final target prediction\n        bully_out = self.bully_fc(aux_features)\n        harmful_out = self.harmful_fc(aux_features)\n        target_out = self.target_fc(aux_features)\n        \n        return sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_out, target_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:58:08.025997Z","iopub.execute_input":"2024-11-24T11:58:08.026580Z","iopub.status.idle":"2024-11-24T11:58:08.038331Z","shell.execute_reply.started":"2024-11-24T11:58:08.026546Z","shell.execute_reply":"2024-11-24T11:58:08.037363Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class Image_text_emotion_sentiment(nn.Module):\n    def __init__(self):\n        super(Image_text_emotion_sentiment, self).__init__()\n        \n        # Visual branch (CNN)\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n        \n        # Textual branch (RoBERTa)\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        \n        # Shared fully connected layers\n        self.fc_shared = nn.Sequential(\n            nn.Linear(2048 + 768, 1024),  # Concatenation of visual and textual features\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),  # Deeper layer\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Task-specific heads\n        self.sentiment_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # Sentiment: 3 classes\n        )\n        \n        self.emotion_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # Emotion: 3 classes\n        )\n        \n        self.sarcasm_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),  # Sarcasm: binary classification\n            nn.Sigmoid()  # Ensures output is in (0, 1)\n        )\n        # self.bully_head = nn.Linear(512, 2)  # Bully: binary classification\n        self.bully_fc = nn.Sequential(\n            nn.Linear(3 + 3 + 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2)  # 2 classes for bully\n        )\n        # self.harmful_head = nn.Linear(512, 3)  # Harmful score: 3 classes\n        self.harmful_fc = nn.Sequential(\n            nn.Linear(3 + 3+ 512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)  # 3 classes for harmful\n        )\n        # Final target head\n        self.target_fc = nn.Sequential(\n            nn.Linear(3 + 3 +  512, 256),  # Input: all task outputs + shared features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 4)  # 4 classes for Target\n        )\n        \n    def forward(self, image, text_input_ids, text_attention_mask):\n        # Visual features\n        img_features = self.resnet(image)\n        \n        # Textual features\n        text_outputs = self.roberta(input_ids=text_input_ids, attention_mask=text_attention_mask)\n        text_features = text_outputs.pooler_output\n        \n        # Concatenate the visual and textual features\n        combined_features = torch.cat((img_features, text_features), dim=1)\n        \n        # Shared features\n        shared_out = self.fc_shared(combined_features)\n        \n        # Task-specific predictions\n        sentiment_out = self.sentiment_head(shared_out)\n        emotion_out = self.emotion_head(shared_out)\n        sarcasm_out = torch.sigmoid(self.sarcasm_head(shared_out))  # Binary\n        # bully_out = self.bully_head(shared_out)\n        # harmful_out = self.harmful_head(shared_out)\n        \n        # Concatenate all task outputs with shared features for target prediction\n        aux_features = torch.cat((\n            emotion_out,\n            sentiment_out,\n            shared_out  # Shared features\n        ), dim=1)\n        \n        # Final target prediction\n        bully_out = self.bully_fc(aux_features)\n        harmful_out = self.harmful_fc(aux_features)\n        target_out = self.target_fc(aux_features)\n        \n        return sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_out, target_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:56:34.633672Z","iopub.execute_input":"2024-11-24T11:56:34.634333Z","iopub.status.idle":"2024-11-24T11:56:34.645639Z","shell.execute_reply.started":"2024-11-24T11:56:34.634301Z","shell.execute_reply":"2024-11-24T11:56:34.644758Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"from torch.utils.data import random_split\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:21:23.465623Z","iopub.execute_input":"2024-11-24T10:21:23.466298Z","iopub.status.idle":"2024-11-24T10:21:23.470107Z","shell.execute_reply.started":"2024-11-24T10:21:23.466268Z","shell.execute_reply":"2024-11-24T10:21:23.469116Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"dataset = MemeDataset(df, transform)\n\n# Create a DataLoader for batching\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Iterate through the dataloader\nfor batch in dataloader:\n    print(f\"Len: {len(batch)}\") \n    # Further model training steps...\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:25:21.508412Z","iopub.execute_input":"2024-11-24T10:25:21.509172Z","iopub.status.idle":"2024-11-24T10:25:21.898142Z","shell.execute_reply.started":"2024-11-24T10:25:21.509138Z","shell.execute_reply":"2024-11-24T10:25:21.897220Z"}},"outputs":[{"name":"stdout","text":"Len: 9\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create the dataset and dataloader\ndataset = MemeDataset(df, transform=transform)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Adjust batch size as needed\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:25:28.141108Z","iopub.execute_input":"2024-11-24T10:25:28.141759Z","iopub.status.idle":"2024-11-24T10:25:28.451342Z","shell.execute_reply.started":"2024-11-24T10:25:28.141728Z","shell.execute_reply":"2024-11-24T10:25:28.450376Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image and text","metadata":{}},{"cell_type":"code","source":"# Initialize the model and move it to the GPU\nmodel= Image_text()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\nmodel.to(device) \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# Loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss().to(device)\nloss_fn_emotion = nn.CrossEntropyLoss().to(device)\nloss_fn_sarcasm = nn.BCEWithLogitsLoss().to(device)\nloss_fn_bully = nn.CrossEntropyLoss().to(device)\nloss_fn_harmful_score = nn.CrossEntropyLoss().to(device)\nloss_fn_target = nn.CrossEntropyLoss().to(device)\n\n# Training loop\nfor epoch in range(15):  # Set epochs accordingly\n    model.train()\n    \n    total_loss = 0  # Initialize total loss for the epoch\n    \n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in train_dataloader:\n        # Move data to the GPU\n    # for images, text_input_ids, text_attention_mask, bully_labels in train_dataloader:\n  \n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n        \n        optimizer.zero_grad()  # Clear gradients at the start of each batch\n        \n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n        \n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())  # Squeeze if necessary\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n        \n        # Total loss (sum or weigh the losses as needed)\n        total_loss_batch = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n        \n        # Backward pass and optimization\n        total_loss_batch.backward()\n        optimizer.step()  # Update model parameters\n        \n        total_loss += total_loss_batch.item()  # Accumulate loss for the epoch\n\n    # Optionally clear cache at the end of each epoch\n    torch.cuda.empty_cache()  \n    \n    # Print the average loss for the epoch\n    avg_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:25:33.094076Z","iopub.execute_input":"2024-11-24T10:25:33.094920Z","iopub.status.idle":"2024-11-24T10:44:13.579435Z","shell.execute_reply.started":"2024-11-24T10:25:33.094846Z","shell.execute_reply":"2024-11-24T10:44:13.578584Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Average Loss: 3.6535\nEpoch 1, Average Loss: 3.3180\nEpoch 2, Average Loss: 3.1197\nEpoch 3, Average Loss: 2.8727\nEpoch 4, Average Loss: 2.5826\nEpoch 5, Average Loss: 2.3852\nEpoch 6, Average Loss: 2.1845\nEpoch 7, Average Loss: 2.0274\nEpoch 8, Average Loss: 1.8677\nEpoch 9, Average Loss: 1.7558\nEpoch 10, Average Loss: 1.6838\nEpoch 11, Average Loss: 1.6833\nEpoch 12, Average Loss: 1.5604\nEpoch 13, Average Loss: 1.5469\nEpoch 14, Average Loss: 1.5032\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:45:43.249817Z","iopub.execute_input":"2024-11-24T10:45:43.250631Z","iopub.status.idle":"2024-11-24T10:45:43.254282Z","shell.execute_reply.started":"2024-11-24T10:45:43.250600Z","shell.execute_reply":"2024-11-24T10:45:43.253344Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def train_model_v1(model, optimizer, losses, epochs=10):\n    # Training loop\n    for epoch in range(epochs):  # Set epochs accordingly\n        model.train()\n        \n        total_loss = 0  # Initialize total loss for the epoch\n        batch_iterator = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{epochs}]')\n        for batch_idx, (images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels) in batch_iterator:\n            # Move data to the GPU\n        # for images, text_input_ids, text_attention_mask, bully_labels in train_dataloader:\n      \n            images = images.to(device)\n            text_input_ids = text_input_ids.to(device)\n            text_attention_mask = text_attention_mask.to(device)\n            sentiment_labels = sentiment_labels.to(device)\n            emotion_labels = emotion_labels.to(device)\n            sarcasm_labels = sarcasm_labels.to(device)\n            bully_labels = bully_labels.to(device)\n            harmful_score_labels = harmful_score_labels.to(device)\n            target_labels = target_labels.to(device)\n            \n            optimizer.zero_grad()  # Clear gradients at the start of each batch\n            \n            # Forward pass\n            sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n            \n            # Compute loss for each task\n            loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n            loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n            loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())  # Squeeze if necessary\n            loss_bully = loss_fn_bully(bully_out, bully_labels)\n            loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n            loss_target = loss_fn_target(target_out, target_labels)\n            \n            # Total loss (sum or weigh the losses as needed)\n            total_loss_batch = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n            \n            # Backward pass and optimization\n            total_loss_batch.backward()\n            optimizer.step()  # Update model parameters\n            \n            total_loss += total_loss_batch.item()  # Accumulate loss for the epoch\n            batch_iterator.set_postfix({'Train Batch Loss': total_loss, 'Training Avg Loss': total_loss_batch.item()/ (batch_idx + 1)})\n        # Optionally clear cache at the end of each epoch\n        torch.cuda.empty_cache()  \n        \n        # Print the average loss for the epoch\n        avg_loss = total_loss / len(train_dataloader)\n        losses.append(avg_loss)\n        print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:46:02.955052Z","iopub.execute_input":"2024-11-24T10:46:02.955626Z","iopub.status.idle":"2024-11-24T10:46:02.964727Z","shell.execute_reply.started":"2024-11-24T10:46:02.955591Z","shell.execute_reply":"2024-11-24T10:46:02.963931Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"losses = []\ntrain_model_v1(model, optimizer, losses, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:46:04.835222Z","iopub.execute_input":"2024-11-24T10:46:04.835508Z","iopub.status.idle":"2024-11-24T10:58:42.302401Z","shell.execute_reply.started":"2024-11-24T10:46:04.835485Z","shell.execute_reply":"2024-11-24T10:58:42.301514Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/10]: 100%|██████████| 154/154 [01:16<00:00,  2.02it/s, Train Batch Loss=212, Training Avg Loss=0.00745]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Average Loss: 1.3758\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/10]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=213, Training Avg Loss=0.0127] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 1.3827\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/10]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=204, Training Avg Loss=0.00701]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 1.3245\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/10]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=205, Training Avg Loss=0.00718]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 1.3282\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/10]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=193, Training Avg Loss=0.0124] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 1.2529\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/10]: 100%|██████████| 154/154 [01:15<00:00,  2.03it/s, Train Batch Loss=189, Training Avg Loss=0.00679]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 1.2285\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/10]: 100%|██████████| 154/154 [01:15<00:00,  2.03it/s, Train Batch Loss=190, Training Avg Loss=0.0125] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 1.2347\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/10]: 100%|██████████| 154/154 [01:15<00:00,  2.03it/s, Train Batch Loss=182, Training Avg Loss=0.00528]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 1.1849\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/10]: 100%|██████████| 154/154 [01:15<00:00,  2.03it/s, Train Batch Loss=183, Training Avg Loss=0.00737]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 1.1896\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/10]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=176, Training Avg Loss=0.00856]","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 1.1438\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:44:13.581258Z","iopub.execute_input":"2024-11-24T10:44:13.582047Z","iopub.status.idle":"2024-11-24T10:44:13.585720Z","shell.execute_reply.started":"2024-11-24T10:44:13.582004Z","shell.execute_reply":"2024-11-24T10:44:13.584909Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Import required libraries (if not already done)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Validation phase\nmodel.eval()  # Set model to evaluation mode\ntotal_val_loss = 0\n\n# Initialize lists to store true and predicted labels for each task\nall_labels_bully = []\nall_preds_bully = []\n\nall_labels_sentiment = []\nall_preds_sentiment = []\n\nall_labels_emotion = []\nall_preds_emotion = []\n\nall_labels_sarcasm = []\nall_preds_sarcasm = []\n\nall_labels_harmful_score = []\nall_preds_harmful_score = []\n\nall_labels_target = []\nall_preds_target = []\n\nwith torch.no_grad():  # Disable gradient calculation\n    batch_iterator = tqdm(val_dataloader, desc=f\"Processing Validation {epoch:02d}\")\n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in batch_iterator:\n        # Move data to the GPU\n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n\n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n\n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n\n        # Total loss\n        total_val_loss += (loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target).item()\n\n        # Get predictions for each task\n        _, predicted_sentiment = torch.max(sentiment_out, 1)\n        _, predicted_emotion = torch.max(emotion_out, 1)\n        _, predicted_sarcasm = torch.max(sarcasm_out, 1)\n        _, predicted_bully = torch.max(bully_out, 1)\n        _, predicted_harmful_score = torch.max(harmful_score_out, 1)  # Assuming multi-class\n        _, predicted_target = torch.max(target_out, 1)  # Assuming multi-class\n\n        # Collect true and predicted labels for each task\n        all_labels_sentiment.append(sentiment_labels.cpu().numpy())\n        all_preds_sentiment.append(predicted_sentiment.cpu().numpy())\n\n        all_labels_emotion.append(emotion_labels.cpu().numpy())\n        all_preds_emotion.append(predicted_emotion.cpu().numpy())\n\n        all_labels_sarcasm.append(sarcasm_labels.cpu().numpy())\n        all_preds_sarcasm.append(predicted_sarcasm.cpu().numpy())\n\n        all_labels_bully.append(bully_labels.cpu().numpy())\n        all_preds_bully.append(predicted_bully.cpu().numpy())\n\n        all_labels_harmful_score.append(harmful_score_labels.cpu().numpy())\n        all_preds_harmful_score.append(predicted_harmful_score.cpu().numpy())\n\n        all_labels_target.append(target_labels.cpu().numpy())\n        all_preds_target.append(predicted_target.cpu().numpy())\n\navg_val_loss = total_val_loss / len(val_dataloader)\n\n# Flatten lists for each task\nall_labels_bully = np.concatenate(all_labels_bully)\nall_preds_bully = np.concatenate(all_preds_bully)\n\nall_labels_sentiment = np.concatenate(all_labels_sentiment)\nall_preds_sentiment = np.concatenate(all_preds_sentiment)\n\nall_labels_emotion = np.concatenate(all_labels_emotion)\nall_preds_emotion = np.concatenate(all_preds_emotion)\n\nall_labels_sarcasm = np.concatenate(all_labels_sarcasm)\nall_preds_sarcasm = np.concatenate(all_preds_sarcasm)\n\nall_labels_harmful_score = np.concatenate(all_labels_harmful_score)\nall_preds_harmful_score = np.concatenate(all_preds_harmful_score)\n\nall_labels_target = np.concatenate(all_labels_target)\nall_preds_target = np.concatenate(all_preds_target)\n\n# Calculate accuracy and F1 score for each task\naccuracy_bully = accuracy_score(all_labels_bully, all_preds_bully)\nf1_bully = f1_score(all_labels_bully, all_preds_bully, average='weighted')\n\naccuracy_sentiment = accuracy_score(all_labels_sentiment, all_preds_sentiment)\nf1_sentiment = f1_score(all_labels_sentiment, all_preds_sentiment, average='weighted')\n\naccuracy_emotion = accuracy_score(all_labels_emotion, all_preds_emotion)\nf1_emotion = f1_score(all_labels_emotion, all_preds_emotion, average='weighted')\n\naccuracy_sarcasm = accuracy_score(all_labels_sarcasm, all_preds_sarcasm)\nf1_sarcasm = f1_score(all_labels_sarcasm, all_preds_sarcasm, average='weighted')\n\naccuracy_harmful_score = accuracy_score(all_labels_harmful_score, all_preds_harmful_score)\nf1_harmful_score = f1_score(all_labels_harmful_score, all_preds_harmful_score, average='weighted')\n\naccuracy_target = accuracy_score(all_labels_target, all_preds_target)\nf1_target = f1_score(all_labels_target, all_preds_target, average='weighted')\n\nprint(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f},\\n'\n      f'Bully Accuracy: {accuracy_bully:.4f}, F1 Score: {f1_bully:.4f},\\n'\n      f'Sentiment Accuracy: {accuracy_sentiment:.4f}, F1 Score: {f1_sentiment:.4f},\\n'\n      f'Emotion Accuracy: {accuracy_emotion:.4f}, F1 Score: {f1_emotion:.4f},\\n'\n      f'Sarcasm Accuracy: {accuracy_sarcasm:.4f}, F1 Score: {f1_sarcasm:.4f},\\n'\n      f'Harmful Score Accuracy: {accuracy_harmful_score:.4f}, F1 Score: {f1_harmful_score:.4f},\\n'\n      f'Target Accuracy: {accuracy_target:.4f}, F1 Score: {f1_target:.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:58:53.406021Z","iopub.execute_input":"2024-11-24T10:58:53.406409Z","iopub.status.idle":"2024-11-24T10:59:00.560284Z","shell.execute_reply.started":"2024-11-24T10:58:53.406358Z","shell.execute_reply":"2024-11-24T10:59:00.559292Z"}},"outputs":[{"name":"stderr","text":"Processing Validation 14: 100%|██████████| 39/39 [00:07<00:00,  5.49it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Validation Loss: 6.4291,\nBully Accuracy: 0.7484, F1 Score: 0.7473,\nSentiment Accuracy: 0.6964, F1 Score: 0.6517,\nEmotion Accuracy: 0.6558, F1 Score: 0.5974,\nSarcasm Accuracy: 0.5081, F1 Score: 0.3424,\nHarmful Score Accuracy: 0.9805, F1 Score: 0.9805,\nTarget Accuracy: 0.7419, F1 Score: 0.7230\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Image_text_emotion","metadata":{}},{"cell_type":"code","source":"# Initialize the model and move it to the GPU\nmodel= Image_text_emotion()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\nmodel.to(device) \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# Loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss().to(device)\nloss_fn_emotion = nn.CrossEntropyLoss().to(device)\nloss_fn_sarcasm = nn.BCEWithLogitsLoss().to(device)\nloss_fn_bully = nn.CrossEntropyLoss().to(device)\nloss_fn_harmful_score = nn.CrossEntropyLoss().to(device)\nloss_fn_target = nn.CrossEntropyLoss().to(device)\n\nepochs = 25\n# Training loop\nfor epoch in range(epochs):  # Set epochs accordingly\n    model.train()\n    \n    total_loss = 0  # Initialize total loss for the epoch\n    \n    batch_iterator = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{epochs}]')\n    for batch_idx, (images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels) in batch_iterator:\n        # Move data to the GPU\n    # for images, text_input_ids, text_attention_mask, bully_labels in train_dataloader:\n  \n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n        \n        optimizer.zero_grad()  # Clear gradients at the start of each batch\n        \n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n        \n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())  # Squeeze if necessary\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n        \n        # Total loss (sum or weigh the losses as needed)\n        total_loss_batch = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n        \n        # Backward pass and optimization\n        total_loss_batch.backward()\n        optimizer.step()  # Update model parameters\n        \n        total_loss += total_loss_batch.item()  # Accumulate loss for the epoch\n        batch_iterator.set_postfix({'Train Batch Loss': total_loss, 'Training Avg Loss': total_loss_batch.item()/ (batch_idx + 1)})\n    # Optionally clear cache at the end of each epoch\n    torch.cuda.empty_cache()  \n    \n    # Print the average loss for the epoch\n    avg_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:37.387573Z","iopub.execute_input":"2024-11-24T11:08:37.387934Z","iopub.status.idle":"2024-11-24T11:40:02.437189Z","shell.execute_reply.started":"2024-11-24T11:08:37.387901Z","shell.execute_reply":"2024-11-24T11:40:02.436289Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch [1/25]: 100%|██████████| 154/154 [01:16<00:00,  2.02it/s, Train Batch Loss=560, Training Avg Loss=0.0165]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Average Loss: 3.6347\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=518, Training Avg Loss=0.0171]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 3.3653\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=494, Training Avg Loss=0.0177]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 3.2073\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=470, Training Avg Loss=0.0184]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 3.0545\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=447, Training Avg Loss=0.0239]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 2.9023\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=425, Training Avg Loss=0.0168]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 2.7600\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=410, Training Avg Loss=0.0137]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 2.6619\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=384, Training Avg Loss=0.0142]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 2.4933\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=366, Training Avg Loss=0.0182]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 2.3795\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=353, Training Avg Loss=0.0144] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 2.2929\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=335, Training Avg Loss=0.0122]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Average Loss: 2.1760\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=323, Training Avg Loss=0.0115] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Average Loss: 2.0994\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=313, Training Avg Loss=0.0131]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Average Loss: 2.0307\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=307, Training Avg Loss=0.0109] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Average Loss: 1.9941\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=297, Training Avg Loss=0.0123]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Average Loss: 1.9265\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=282, Training Avg Loss=0.0115] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Average Loss: 1.8280\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=276, Training Avg Loss=0.0122] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Average Loss: 1.7919\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=273, Training Avg Loss=0.0141] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Average Loss: 1.7712\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=267, Training Avg Loss=0.0119] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Average Loss: 1.7331\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=250, Training Avg Loss=0.00852]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Average Loss: 1.6254\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=243, Training Avg Loss=0.0129] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Average Loss: 1.5790\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=244, Training Avg Loss=0.0145] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Average Loss: 1.5852\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=238, Training Avg Loss=0.00663]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Average Loss: 1.5422\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=236, Training Avg Loss=0.0115] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Average Loss: 1.5344\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=227, Training Avg Loss=0.0205] ","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Average Loss: 1.4760\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Import required libraries (if not already done)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Validation phase\nmodel.eval()  # Set model to evaluation mode\ntotal_val_loss = 0\n\n# Initialize lists to store true and predicted labels for each task\nall_labels_bully = []\nall_preds_bully = []\n\nall_labels_sentiment = []\nall_preds_sentiment = []\n\nall_labels_emotion = []\nall_preds_emotion = []\n\nall_labels_sarcasm = []\nall_preds_sarcasm = []\n\nall_labels_harmful_score = []\nall_preds_harmful_score = []\n\nall_labels_target = []\nall_preds_target = []\n\nwith torch.no_grad():  # Disable gradient calculation\n    batch_iterator = tqdm(val_dataloader, desc=f\"Processing Validation {epoch:02d}\")\n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in batch_iterator:\n        # Move data to the GPU\n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n\n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n\n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n\n        # Total loss\n        total_val_loss += (loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target).item()\n\n        # Get predictions for each task\n        _, predicted_sentiment = torch.max(sentiment_out, 1)\n        _, predicted_emotion = torch.max(emotion_out, 1)\n        _, predicted_sarcasm = torch.max(sarcasm_out, 1)\n        _, predicted_bully = torch.max(bully_out, 1)\n        _, predicted_harmful_score = torch.max(harmful_score_out, 1)  # Assuming multi-class\n        _, predicted_target = torch.max(target_out, 1)  # Assuming multi-class\n\n        # Collect true and predicted labels for each task\n        all_labels_sentiment.append(sentiment_labels.cpu().numpy())\n        all_preds_sentiment.append(predicted_sentiment.cpu().numpy())\n\n        all_labels_emotion.append(emotion_labels.cpu().numpy())\n        all_preds_emotion.append(predicted_emotion.cpu().numpy())\n\n        all_labels_sarcasm.append(sarcasm_labels.cpu().numpy())\n        all_preds_sarcasm.append(predicted_sarcasm.cpu().numpy())\n\n        all_labels_bully.append(bully_labels.cpu().numpy())\n        all_preds_bully.append(predicted_bully.cpu().numpy())\n\n        all_labels_harmful_score.append(harmful_score_labels.cpu().numpy())\n        all_preds_harmful_score.append(predicted_harmful_score.cpu().numpy())\n\n        all_labels_target.append(target_labels.cpu().numpy())\n        all_preds_target.append(predicted_target.cpu().numpy())\n\navg_val_loss = total_val_loss / len(val_dataloader)\n\n# Flatten lists for each task\nall_labels_bully = np.concatenate(all_labels_bully)\nall_preds_bully = np.concatenate(all_preds_bully)\n\nall_labels_sentiment = np.concatenate(all_labels_sentiment)\nall_preds_sentiment = np.concatenate(all_preds_sentiment)\n\nall_labels_emotion = np.concatenate(all_labels_emotion)\nall_preds_emotion = np.concatenate(all_preds_emotion)\n\nall_labels_sarcasm = np.concatenate(all_labels_sarcasm)\nall_preds_sarcasm = np.concatenate(all_preds_sarcasm)\n\nall_labels_harmful_score = np.concatenate(all_labels_harmful_score)\nall_preds_harmful_score = np.concatenate(all_preds_harmful_score)\n\nall_labels_target = np.concatenate(all_labels_target)\nall_preds_target = np.concatenate(all_preds_target)\n\n# Calculate accuracy and F1 score for each task\naccuracy_bully_em = accuracy_score(all_labels_bully, all_preds_bully)\nf1_bully_em = f1_score(all_labels_bully, all_preds_bully, average='weighted')\n\naccuracy_sentiment_em = accuracy_score(all_labels_sentiment, all_preds_sentiment)\nf1_sentiment_em = f1_score(all_labels_sentiment, all_preds_sentiment, average='weighted')\n\naccuracy_emotion_em = accuracy_score(all_labels_emotion, all_preds_emotion)\nf1_emotion_em = f1_score(all_labels_emotion, all_preds_emotion, average='weighted')\n\naccuracy_sarcasm_em = accuracy_score(all_labels_sarcasm, all_preds_sarcasm)\nf1_sarcasm_em = f1_score(all_labels_sarcasm, all_preds_sarcasm, average='weighted')\n\naccuracy_harmful_score_em = accuracy_score(all_labels_harmful_score, all_preds_harmful_score)\nf1_harmful_score_em = f1_score(all_labels_harmful_score, all_preds_harmful_score, average='weighted')\n\naccuracy_target_em = accuracy_score(all_labels_target, all_preds_target)\nf1_target_em = f1_score(all_labels_target, all_preds_target, average='weighted')\n\nprint(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f},\\n'\n      f'Bully Accuracy: {accuracy_bully_em:.4f}, F1 Score: {f1_bully_em:.4f},\\n'\n      f'Sentiment Accuracy: {accuracy_sentiment_em:.4f}, F1 Score: {f1_sentiment_em:.4f},\\n'\n      f'Emotion Accuracy: {accuracy_emotion_em:.4f}, F1 Score: {f1_emotion_em:.4f},\\n'\n      f'Sarcasm Accuracy: {accuracy_sarcasm_em:.4f}, F1 Score: {f1_sarcasm_em:.4f},\\n'\n      f'Harmful Score Accuracy: {accuracy_harmful_score_em:.4f}, F1 Score: {f1_harmful_score_em:.4f},\\n'\n      f'Target Accuracy: {accuracy_target_em:.4f}, F1 Score: {f1_target_em:.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:40:02.439278Z","iopub.execute_input":"2024-11-24T11:40:02.439642Z","iopub.status.idle":"2024-11-24T11:40:09.581179Z","shell.execute_reply.started":"2024-11-24T11:40:02.439604Z","shell.execute_reply":"2024-11-24T11:40:09.580272Z"}},"outputs":[{"name":"stderr","text":"Processing Validation 24: 100%|██████████| 39/39 [00:07<00:00,  5.49it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Validation Loss: 6.1328,\nBully Accuracy: 0.7500, F1 Score: 0.7633,\nSentiment Accuracy: 0.6688, F1 Score: 0.6589,\nEmotion Accuracy: 0.6039, F1 Score: 0.5888,\nSarcasm Accuracy: 0.5081, F1 Score: 0.3424,\nHarmful Score Accuracy: 0.9870, F1 Score: 0.9838,\nTarget Accuracy: 0.7208, F1 Score: 0.7150\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"# Image_text_sentiment","metadata":{}},{"cell_type":"code","source":"# Initialize the model and move it to the GPU\nmodel= Image_text_sentiment()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\nmodel.to(device) \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# Loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss().to(device)\nloss_fn_emotion = nn.CrossEntropyLoss().to(device)\nloss_fn_sarcasm = nn.BCEWithLogitsLoss().to(device)\nloss_fn_bully = nn.CrossEntropyLoss().to(device)\nloss_fn_harmful_score = nn.CrossEntropyLoss().to(device)\nloss_fn_target = nn.CrossEntropyLoss().to(device)\n\nepochs = 25\n# Training loop\nfor epoch in range(epochs):  # Set epochs accordingly\n    model.train()\n    \n    total_loss = 0  # Initialize total loss for the epoch\n    batch_iterator = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{epochs}]')\n    for batch_idx, (images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels) in batch_iterator:\n        # Move data to the GPU\n    # for images, text_input_ids, text_attention_mask, bully_labels in train_dataloader:\n  \n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n        \n        optimizer.zero_grad()  # Clear gradients at the start of each batch\n        \n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n        \n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())  # Squeeze if necessary\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n        \n        # Total loss (sum or weigh the losses as needed)\n        total_loss_batch = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n        \n        # Backward pass and optimization\n        total_loss_batch.backward()\n        optimizer.step()  # Update model parameters\n        \n        total_loss += total_loss_batch.item()  # Accumulate loss for the epoch\n        batch_iterator.set_postfix({'Train Batch Loss': total_loss, 'Training Avg Loss': total_loss_batch.item()/ (batch_idx + 1)})\n    # Optionally clear cache at the end of each epoch\n    torch.cuda.empty_cache()  \n    \n    # Print the average loss for the epoch\n    avg_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:58:17.740852Z","iopub.execute_input":"2024-11-24T11:58:17.741225Z","iopub.status.idle":"2024-11-24T12:29:45.889443Z","shell.execute_reply.started":"2024-11-24T11:58:17.741196Z","shell.execute_reply":"2024-11-24T12:29:45.888441Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch [1/25]:   0%|          | 0/154 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nEpoch [1/25]: 100%|██████████| 154/154 [01:16<00:00,  2.02it/s, Train Batch Loss=568, Training Avg Loss=0.0233]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Average Loss: 3.6867\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/25]: 100%|██████████| 154/154 [01:15<00:00,  2.03it/s, Train Batch Loss=520, Training Avg Loss=0.0252]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 3.3748\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=500, Training Avg Loss=0.0292]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 3.2467\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=477, Training Avg Loss=0.019] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 3.1002\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=452, Training Avg Loss=0.0191]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 2.9364\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=434, Training Avg Loss=0.0245]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 2.8172\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=417, Training Avg Loss=0.0234]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 2.7107\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=394, Training Avg Loss=0.0154]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 2.5597\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=384, Training Avg Loss=0.0234]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 2.4918\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=373, Training Avg Loss=0.0169]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 2.4197\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=360, Training Avg Loss=0.0148]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Average Loss: 2.3352\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=339, Training Avg Loss=0.0174]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Average Loss: 2.2011\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=335, Training Avg Loss=0.0154] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Average Loss: 2.1761\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=324, Training Avg Loss=0.0123]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Average Loss: 2.1016\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=306, Training Avg Loss=0.011]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Average Loss: 1.9898\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=298, Training Avg Loss=0.0117] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Average Loss: 1.9380\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=296, Training Avg Loss=0.019]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Average Loss: 1.9201\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=286, Training Avg Loss=0.00952]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Average Loss: 1.8594\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=273, Training Avg Loss=0.0104] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Average Loss: 1.7718\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=264, Training Avg Loss=0.0116] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Average Loss: 1.7170\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=258, Training Avg Loss=0.0116] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Average Loss: 1.6763\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=248, Training Avg Loss=0.0145] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Average Loss: 1.6108\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=247, Training Avg Loss=0.00864]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Average Loss: 1.6017\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=236, Training Avg Loss=0.00911]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Average Loss: 1.5312\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=229, Training Avg Loss=0.0125] ","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Average Loss: 1.4880\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# Import required libraries (if not already done)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Validation phase\nmodel.eval()  # Set model to evaluation mode\ntotal_val_loss = 0\n\n# Initialize lists to store true and predicted labels for each task\nall_labels_bully = []\nall_preds_bully = []\n\nall_labels_sentiment = []\nall_preds_sentiment = []\n\nall_labels_emotion = []\nall_preds_emotion = []\n\nall_labels_sarcasm = []\nall_preds_sarcasm = []\n\nall_labels_harmful_score = []\nall_preds_harmful_score = []\n\nall_labels_target = []\nall_preds_target = []\n\nwith torch.no_grad():  # Disable gradient calculation\n    batch_iterator = tqdm(val_dataloader, desc=f\"Processing Validation {epoch:02d}\")\n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in batch_iterator:\n        # Move data to the GPU\n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n\n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n\n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n\n        # Total loss\n        total_val_loss += (loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target).item()\n\n        # Get predictions for each task\n        _, predicted_sentiment = torch.max(sentiment_out, 1)\n        _, predicted_emotion = torch.max(emotion_out, 1)\n        _, predicted_sarcasm = torch.max(sarcasm_out, 1)\n        _, predicted_bully = torch.max(bully_out, 1)\n        _, predicted_harmful_score = torch.max(harmful_score_out, 1)  # Assuming multi-class\n        _, predicted_target = torch.max(target_out, 1)  # Assuming multi-class\n\n        # Collect true and predicted labels for each task\n        all_labels_sentiment.append(sentiment_labels.cpu().numpy())\n        all_preds_sentiment.append(predicted_sentiment.cpu().numpy())\n\n        all_labels_emotion.append(emotion_labels.cpu().numpy())\n        all_preds_emotion.append(predicted_emotion.cpu().numpy())\n\n        all_labels_sarcasm.append(sarcasm_labels.cpu().numpy())\n        all_preds_sarcasm.append(predicted_sarcasm.cpu().numpy())\n\n        all_labels_bully.append(bully_labels.cpu().numpy())\n        all_preds_bully.append(predicted_bully.cpu().numpy())\n\n        all_labels_harmful_score.append(harmful_score_labels.cpu().numpy())\n        all_preds_harmful_score.append(predicted_harmful_score.cpu().numpy())\n\n        all_labels_target.append(target_labels.cpu().numpy())\n        all_preds_target.append(predicted_target.cpu().numpy())\n\navg_val_loss = total_val_loss / len(val_dataloader)\n\n# Flatten lists for each task\nall_labels_bully = np.concatenate(all_labels_bully)\nall_preds_bully = np.concatenate(all_preds_bully)\n\nall_labels_sentiment = np.concatenate(all_labels_sentiment)\nall_preds_sentiment = np.concatenate(all_preds_sentiment)\n\nall_labels_emotion = np.concatenate(all_labels_emotion)\nall_preds_emotion = np.concatenate(all_preds_emotion)\n\nall_labels_sarcasm = np.concatenate(all_labels_sarcasm)\nall_preds_sarcasm = np.concatenate(all_preds_sarcasm)\n\nall_labels_harmful_score = np.concatenate(all_labels_harmful_score)\nall_preds_harmful_score = np.concatenate(all_preds_harmful_score)\n\nall_labels_target = np.concatenate(all_labels_target)\nall_preds_target = np.concatenate(all_preds_target)\n\n# Calculate accuracy and F1 score for each task\naccuracy_bully_SA = accuracy_score(all_labels_bully, all_preds_bully)\nf1_bully_SA = f1_score(all_labels_bully, all_preds_bully, average='weighted')\n\naccuracy_sentiment_SA = accuracy_score(all_labels_sentiment, all_preds_sentiment)\nf1_sentiment_SA = f1_score(all_labels_sentiment, all_preds_sentiment, average='weighted')\n\naccuracy_emotion_SA = accuracy_score(all_labels_emotion, all_preds_emotion)\nf1_emotion_SA = f1_score(all_labels_emotion, all_preds_emotion, average='weighted')\n\naccuracy_sarcasm_SA = accuracy_score(all_labels_sarcasm, all_preds_sarcasm)\nf1_sarcasm_SA = f1_score(all_labels_sarcasm, all_preds_sarcasm, average='weighted')\n\naccuracy_harmful_score_SA = accuracy_score(all_labels_harmful_score, all_preds_harmful_score)\nf1_harmful_score_SA = f1_score(all_labels_harmful_score, all_preds_harmful_score, average='weighted')\n\naccuracy_target_SA = accuracy_score(all_labels_target, all_preds_target)\nf1_target_SA = f1_score(all_labels_target, all_preds_target, average='weighted')\n\nprint(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f},\\n'\n      f'Bully Accuracy: {accuracy_bully_SA:.4f}, F1 Score: {f1_bully_SA:.4f},\\n'\n      f'Sentiment Accuracy: {accuracy_sentiment_SA:.4f}, F1 Score: {f1_sentiment_SA:.4f},\\n'\n      f'Emotion Accuracy: {accuracy_emotion_SA:.4f}, F1 Score: {f1_emotion_SA:.4f},\\n'\n      f'Sarcasm Accuracy: {accuracy_sarcasm_SA:.4f}, F1 Score: {f1_sarcasm_SA:.4f},\\n'\n      f'Harmful Score Accuracy: {accuracy_harmful_score_SA:.4f}, F1 Score: {f1_harmful_score_SA:.4f},\\n'\n      f'Target Accuracy: {accuracy_target_SA:.4f}, F1 Score: {f1_target_SA:.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T12:29:45.890901Z","iopub.execute_input":"2024-11-24T12:29:45.891166Z","iopub.status.idle":"2024-11-24T12:29:53.143209Z","shell.execute_reply.started":"2024-11-24T12:29:45.891140Z","shell.execute_reply":"2024-11-24T12:29:53.142333Z"}},"outputs":[{"name":"stderr","text":"Processing Validation 24: 100%|██████████| 39/39 [00:07<00:00,  5.40it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Validation Loss: 5.5873,\nBully Accuracy: 0.7403, F1 Score: 0.7470,\nSentiment Accuracy: 0.6997, F1 Score: 0.6693,\nEmotion Accuracy: 0.6153, F1 Score: 0.6042,\nSarcasm Accuracy: 0.5081, F1 Score: 0.3424,\nHarmful Score Accuracy: 0.9838, F1 Score: 0.9822,\nTarget Accuracy: 0.7500, F1 Score: 0.7199\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"# Image_text_emotion sentiment","metadata":{}},{"cell_type":"code","source":"# Initialize the model and move it to the GPU\nmodel= Image_text_emotion_sentiment()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\nmodel.to(device) \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# Loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss().to(device)\nloss_fn_emotion = nn.CrossEntropyLoss().to(device)\nloss_fn_sarcasm = nn.BCEWithLogitsLoss().to(device)\nloss_fn_bully = nn.CrossEntropyLoss().to(device)\nloss_fn_harmful_score = nn.CrossEntropyLoss().to(device)\nloss_fn_target = nn.CrossEntropyLoss().to(device)\n\nepochs = 25\n# Training loop\nfor epoch in range(epochs):  # Set epochs accordingly\n    model.train()\n    \n    total_loss = 0  # Initialize total loss for the epoch\n    batch_iterator = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{epochs}]')\n    for batch_idx, (images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels) in batch_iterator:\n        # Move data to the GPU\n    # for images, text_input_ids, text_attention_mask, bully_labels in train_dataloader:\n  \n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n        \n        optimizer.zero_grad()  # Clear gradients at the start of each batch\n        \n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n        \n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())  # Squeeze if necessary\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n        \n        # Total loss (sum or weigh the losses as needed)\n        total_loss_batch = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n        \n        # Backward pass and optimization\n        total_loss_batch.backward()\n        optimizer.step()  # Update model parameters\n        \n        total_loss += total_loss_batch.item()  # Accumulate loss for the epoch\n        batch_iterator.set_postfix({'Train Batch Loss': total_loss, 'Training Avg Loss': total_loss_batch.item()/ (batch_idx + 1)})\n    # Optionally clear cache at the end of each epoch\n    torch.cuda.empty_cache()  \n    \n    # Print the average loss for the epoch\n    avg_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T12:29:53.144728Z","iopub.execute_input":"2024-11-24T12:29:53.145586Z","iopub.status.idle":"2024-11-24T13:01:16.042882Z","shell.execute_reply.started":"2024-11-24T12:29:53.145541Z","shell.execute_reply":"2024-11-24T13:01:16.041828Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch [1/25]:   0%|          | 0/154 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nEpoch [1/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=559, Training Avg Loss=0.0218]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Average Loss: 3.6295\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=519, Training Avg Loss=0.0207]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 3.3689\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=494, Training Avg Loss=0.0244]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 3.2099\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=471, Training Avg Loss=0.0182]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 3.0586\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=442, Training Avg Loss=0.0186]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 2.8724\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=421, Training Avg Loss=0.0138]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 2.7332\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=402, Training Avg Loss=0.0156]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 2.6103\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=376, Training Avg Loss=0.0211]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 2.4428\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=359, Training Avg Loss=0.0239]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 2.3315\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=349, Training Avg Loss=0.0135]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 2.2691\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=328, Training Avg Loss=0.0121]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Average Loss: 2.1278\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=324, Training Avg Loss=0.0176]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Average Loss: 2.1058\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=318, Training Avg Loss=0.0141]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Average Loss: 2.0619\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=293, Training Avg Loss=0.0118] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Average Loss: 1.8996\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=291, Training Avg Loss=0.0112] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Average Loss: 1.8922\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=289, Training Avg Loss=0.0122]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Average Loss: 1.8736\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=271, Training Avg Loss=0.0119] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Average Loss: 1.7598\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=265, Training Avg Loss=0.0177] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Average Loss: 1.7190\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=255, Training Avg Loss=0.0111] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Average Loss: 1.6547\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=264, Training Avg Loss=0.00716]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Average Loss: 1.7165\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=249, Training Avg Loss=0.00844]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Average Loss: 1.6172\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=244, Training Avg Loss=0.00852]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Average Loss: 1.5870\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=241, Training Avg Loss=0.0122] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Average Loss: 1.5660\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/25]: 100%|██████████| 154/154 [01:15<00:00,  2.05it/s, Train Batch Loss=238, Training Avg Loss=0.00976]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Average Loss: 1.5430\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/25]: 100%|██████████| 154/154 [01:15<00:00,  2.04it/s, Train Batch Loss=224, Training Avg Loss=0.00877]","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Average Loss: 1.4535\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"# Import required libraries (if not already done)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Validation phase\nmodel.eval()  # Set model to evaluation mode\ntotal_val_loss = 0\n\n# Initialize lists to store true and predicted labels for each task\nall_labels_bully = []\nall_preds_bully = []\n\nall_labels_sentiment = []\nall_preds_sentiment = []\n\nall_labels_emotion = []\nall_preds_emotion = []\n\nall_labels_sarcasm = []\nall_preds_sarcasm = []\n\nall_labels_harmful_score = []\nall_preds_harmful_score = []\n\nall_labels_target = []\nall_preds_target = []\n\nwith torch.no_grad():  # Disable gradient calculation\n    batch_iterator = tqdm(val_dataloader, desc=f\"Processing Validation {epoch:02d}\")\n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in batch_iterator:\n        # Move data to the GPU\n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n\n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n\n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n\n        # Total loss\n        total_val_loss += (loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target).item()\n\n        # Get predictions for each task\n        _, predicted_sentiment = torch.max(sentiment_out, 1)\n        _, predicted_emotion = torch.max(emotion_out, 1)\n        _, predicted_sarcasm = torch.max(sarcasm_out, 1)\n        _, predicted_bully = torch.max(bully_out, 1)\n        _, predicted_harmful_score = torch.max(harmful_score_out, 1)  # Assuming multi-class\n        _, predicted_target = torch.max(target_out, 1)  # Assuming multi-class\n\n        # Collect true and predicted labels for each task\n        all_labels_sentiment.append(sentiment_labels.cpu().numpy())\n        all_preds_sentiment.append(predicted_sentiment.cpu().numpy())\n\n        all_labels_emotion.append(emotion_labels.cpu().numpy())\n        all_preds_emotion.append(predicted_emotion.cpu().numpy())\n\n        all_labels_sarcasm.append(sarcasm_labels.cpu().numpy())\n        all_preds_sarcasm.append(predicted_sarcasm.cpu().numpy())\n\n        all_labels_bully.append(bully_labels.cpu().numpy())\n        all_preds_bully.append(predicted_bully.cpu().numpy())\n\n        all_labels_harmful_score.append(harmful_score_labels.cpu().numpy())\n        all_preds_harmful_score.append(predicted_harmful_score.cpu().numpy())\n\n        all_labels_target.append(target_labels.cpu().numpy())\n        all_preds_target.append(predicted_target.cpu().numpy())\n\navg_val_loss = total_val_loss / len(val_dataloader)\n\n# Flatten lists for each task\nall_labels_bully = np.concatenate(all_labels_bully)\nall_preds_bully = np.concatenate(all_preds_bully)\n\nall_labels_sentiment = np.concatenate(all_labels_sentiment)\nall_preds_sentiment = np.concatenate(all_preds_sentiment)\n\nall_labels_emotion = np.concatenate(all_labels_emotion)\nall_preds_emotion = np.concatenate(all_preds_emotion)\n\nall_labels_sarcasm = np.concatenate(all_labels_sarcasm)\nall_preds_sarcasm = np.concatenate(all_preds_sarcasm)\n\nall_labels_harmful_score = np.concatenate(all_labels_harmful_score)\nall_preds_harmful_score = np.concatenate(all_preds_harmful_score)\n\nall_labels_target = np.concatenate(all_labels_target)\nall_preds_target = np.concatenate(all_preds_target)\n\n# Calculate accuracy and F1 score for each task\naccuracy_bully_SA_EM = accuracy_score(all_labels_bully, all_preds_bully)\nf1_bully_SA_EM = f1_score(all_labels_bully, all_preds_bully, average='weighted')\n\naccuracy_sentiment_SA_EM = accuracy_score(all_labels_sentiment, all_preds_sentiment)\nf1_sentiment_SA_EM = f1_score(all_labels_sentiment, all_preds_sentiment, average='weighted')\n\naccuracy_emotion_SA_EM = accuracy_score(all_labels_emotion, all_preds_emotion)\nf1_emotion_SA_EM = f1_score(all_labels_emotion, all_preds_emotion, average='weighted')\n\naccuracy_sarcasm_SA_EM = accuracy_score(all_labels_sarcasm, all_preds_sarcasm)\nf1_sarcasm_SA_EM = f1_score(all_labels_sarcasm, all_preds_sarcasm, average='weighted')\n\naccuracy_harmful_score_SA_EM = accuracy_score(all_labels_harmful_score, all_preds_harmful_score)\nf1_harmful_score_SA_EM = f1_score(all_labels_harmful_score, all_preds_harmful_score, average='weighted')\n\naccuracy_target_SA_EM = accuracy_score(all_labels_target, all_preds_target)\nf1_target_SA_EM = f1_score(all_labels_target, all_preds_target, average='weighted')\n\nprint(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f},\\n'\n      f'Bully Accuracy: {accuracy_bully_SA_EM:.4f}, F1 Score: {f1_bully_SA_EM:.4f},\\n'\n      f'Sentiment Accuracy: {accuracy_sentiment_SA_EM:.4f}, F1 Score: {f1_sentiment_SA_EM:.4f},\\n'\n      f'Emotion Accuracy: {accuracy_emotion_SA_EM:.4f}, F1 Score: {f1_emotion_SA_EM:.4f},\\n'\n      f'Sarcasm Accuracy: {accuracy_sarcasm_SA_EM:.4f}, F1 Score: {f1_sarcasm_SA_EM:.4f},\\n'\n      f'Harmful Score Accuracy: {accuracy_harmful_score_SA_EM:.4f}, F1 Score: {f1_harmful_score_SA_EM:.4f},\\n'\n      f'Target Accuracy: {accuracy_target_SA_EM:.4f}, F1 Score: {f1_target_SA_EM:.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:01:16.045041Z","iopub.execute_input":"2024-11-24T13:01:16.045314Z","iopub.status.idle":"2024-11-24T13:01:23.136160Z","shell.execute_reply.started":"2024-11-24T13:01:16.045287Z","shell.execute_reply":"2024-11-24T13:01:23.135200Z"}},"outputs":[{"name":"stderr","text":"Processing Validation 24: 100%|██████████| 39/39 [00:07<00:00,  5.53it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Validation Loss: 6.7434,\nBully Accuracy: 0.7727, F1 Score: 0.7591,\nSentiment Accuracy: 0.5438, F1 Score: 0.5711,\nEmotion Accuracy: 0.5649, F1 Score: 0.5656,\nSarcasm Accuracy: 0.5081, F1 Score: 0.3424,\nHarmful Score Accuracy: 0.9854, F1 Score: 0.9830,\nTarget Accuracy: 0.7062, F1 Score: 0.7003\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}